{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport gc\nimport json\nimport math\nimport cv2\nimport PIL\nimport re\nfrom PIL import Image\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nimport matplotlib.pyplot as plt\n#from sklearn.metrics import cohen_kappa_score, accuracy_score\nimport scipy\nfrom tqdm import tqdm\n%matplotlib inline\n#from keras.preprocessing import image\nimport glob\nimport tensorflow.keras.applications.densenet as dense\nfrom kaggle_datasets import KaggleDatasets\nimport seaborn as sns\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-01T12:08:08.051206Z","iopub.execute_input":"2021-09-01T12:08:08.051502Z","iopub.status.idle":"2021-09-01T12:08:13.926903Z","shell.execute_reply.started":"2021-09-01T12:08:08.051432Z","shell.execute_reply":"2021-09-01T12:08:13.925998Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"tf.__version__","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-09-01T12:08:21.557388Z","iopub.execute_input":"2021-09-01T12:08:21.557701Z","iopub.status.idle":"2021-09-01T12:08:21.566730Z","shell.execute_reply.started":"2021-09-01T12:08:21.557672Z","shell.execute_reply":"2021-09-01T12:08:21.565766Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'2.4.1'"},"metadata":{}}]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/siim-isic-melanoma-classification/train.csv')\ntest = pd.read_csv('/kaggle/input/siim-isic-melanoma-classification/test.csv')\n\nprint('Train: ', train.shape)\nprint(\"Test:\", test.shape)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-01T12:08:27.387305Z","iopub.execute_input":"2021-09-01T12:08:27.387631Z","iopub.status.idle":"2021-09-01T12:08:27.525604Z","shell.execute_reply.started":"2021-09-01T12:08:27.387601Z","shell.execute_reply":"2021-09-01T12:08:27.524536Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Train:  (33126, 8)\nTest: (10982, 5)\n","output_type":"stream"}]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T12:08:30.232691Z","iopub.execute_input":"2021-09-01T12:08:30.233078Z","iopub.status.idle":"2021-09-01T12:08:30.257629Z","shell.execute_reply.started":"2021-09-01T12:08:30.233049Z","shell.execute_reply":"2021-09-01T12:08:30.256511Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"     image_name  patient_id     sex  age_approx anatom_site_general_challenge  \\\n0  ISIC_2637011  IP_7279968    male        45.0                     head/neck   \n1  ISIC_0015719  IP_3075186  female        45.0               upper extremity   \n2  ISIC_0052212  IP_2842074  female        50.0               lower extremity   \n3  ISIC_0068279  IP_6890425  female        45.0                     head/neck   \n4  ISIC_0074268  IP_8723313  female        55.0               upper extremity   \n\n  diagnosis benign_malignant  target  \n0   unknown           benign       0  \n1   unknown           benign       0  \n2     nevus           benign       0  \n3   unknown           benign       0  \n4   unknown           benign       0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_name</th>\n      <th>patient_id</th>\n      <th>sex</th>\n      <th>age_approx</th>\n      <th>anatom_site_general_challenge</th>\n      <th>diagnosis</th>\n      <th>benign_malignant</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ISIC_2637011</td>\n      <td>IP_7279968</td>\n      <td>male</td>\n      <td>45.0</td>\n      <td>head/neck</td>\n      <td>unknown</td>\n      <td>benign</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ISIC_0015719</td>\n      <td>IP_3075186</td>\n      <td>female</td>\n      <td>45.0</td>\n      <td>upper extremity</td>\n      <td>unknown</td>\n      <td>benign</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ISIC_0052212</td>\n      <td>IP_2842074</td>\n      <td>female</td>\n      <td>50.0</td>\n      <td>lower extremity</td>\n      <td>nevus</td>\n      <td>benign</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ISIC_0068279</td>\n      <td>IP_6890425</td>\n      <td>female</td>\n      <td>45.0</td>\n      <td>head/neck</td>\n      <td>unknown</td>\n      <td>benign</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ISIC_0074268</td>\n      <td>IP_8723313</td>\n      <td>female</td>\n      <td>55.0</td>\n      <td>upper extremity</td>\n      <td>unknown</td>\n      <td>benign</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T12:08:35.727885Z","iopub.execute_input":"2021-09-01T12:08:35.728197Z","iopub.status.idle":"2021-09-01T12:08:35.760143Z","shell.execute_reply.started":"2021-09-01T12:08:35.728169Z","shell.execute_reply":"2021-09-01T12:08:35.759245Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 33126 entries, 0 to 33125\nData columns (total 8 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   image_name                     33126 non-null  object \n 1   patient_id                     33126 non-null  object \n 2   sex                            33061 non-null  object \n 3   age_approx                     33058 non-null  float64\n 4   anatom_site_general_challenge  32599 non-null  object \n 5   diagnosis                      33126 non-null  object \n 6   benign_malignant               33126 non-null  object \n 7   target                         33126 non-null  int64  \ndtypes: float64(1), int64(1), object(6)\nmemory usage: 2.0+ MB\n","output_type":"stream"}]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T12:08:38.520527Z","iopub.execute_input":"2021-09-01T12:08:38.520954Z","iopub.status.idle":"2021-09-01T12:08:38.543112Z","shell.execute_reply.started":"2021-09-01T12:08:38.520915Z","shell.execute_reply":"2021-09-01T12:08:38.542200Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"     image_name  patient_id     sex  age_approx anatom_site_general_challenge\n0  ISIC_0052060  IP_3579794    male        70.0                           NaN\n1  ISIC_0052349  IP_7782715    male        40.0               lower extremity\n2  ISIC_0058510  IP_7960270  female        55.0                         torso\n3  ISIC_0073313  IP_6375035  female        50.0                         torso\n4  ISIC_0073502  IP_0589375  female        45.0               lower extremity","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_name</th>\n      <th>patient_id</th>\n      <th>sex</th>\n      <th>age_approx</th>\n      <th>anatom_site_general_challenge</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ISIC_0052060</td>\n      <td>IP_3579794</td>\n      <td>male</td>\n      <td>70.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ISIC_0052349</td>\n      <td>IP_7782715</td>\n      <td>male</td>\n      <td>40.0</td>\n      <td>lower extremity</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ISIC_0058510</td>\n      <td>IP_7960270</td>\n      <td>female</td>\n      <td>55.0</td>\n      <td>torso</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ISIC_0073313</td>\n      <td>IP_6375035</td>\n      <td>female</td>\n      <td>50.0</td>\n      <td>torso</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ISIC_0073502</td>\n      <td>IP_0589375</td>\n      <td>female</td>\n      <td>45.0</td>\n      <td>lower extremity</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test.info()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T12:08:40.816563Z","iopub.execute_input":"2021-09-01T12:08:40.816926Z","iopub.status.idle":"2021-09-01T12:08:40.833407Z","shell.execute_reply.started":"2021-09-01T12:08:40.816892Z","shell.execute_reply":"2021-09-01T12:08:40.832113Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10982 entries, 0 to 10981\nData columns (total 5 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   image_name                     10982 non-null  object \n 1   patient_id                     10982 non-null  object \n 2   sex                            10982 non-null  object \n 3   age_approx                     10982 non-null  float64\n 4   anatom_site_general_challenge  10631 non-null  object \ndtypes: float64(1), object(4)\nmemory usage: 429.1+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"train['benign_malignant'].value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T12:08:43.848882Z","iopub.execute_input":"2021-09-01T12:08:43.849244Z","iopub.status.idle":"2021-09-01T12:08:43.878005Z","shell.execute_reply.started":"2021-09-01T12:08:43.849204Z","shell.execute_reply":"2021-09-01T12:08:43.877270Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"benign       0.98237\nmalignant    0.01763\nName: benign_malignant, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"#sns.countplot(train['benign_malignant'])","metadata":{"execution":{"iopub.status.busy":"2021-09-01T12:08:46.088150Z","iopub.execute_input":"2021-09-01T12:08:46.088471Z","iopub.status.idle":"2021-09-01T12:08:46.092919Z","shell.execute_reply.started":"2021-09-01T12:08:46.088442Z","shell.execute_reply":"2021-09-01T12:08:46.091816Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#train['sex'].value_counts(normalize=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train['target'].groupby(train['sex']).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sns.countplot(train['sex'], hue=train['target'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['target'].groupby(train['age_approx']).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plt.figure(figsize=(8,5))\n#sns.countplot(train['age_approx'], hue=train['target'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train['anatom_site_general_challenge'].value_counts(normalize=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train['target'].groupby(train['anatom_site_general_challenge']).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.figure(figsize=(10,5))\n# sns.countplot(train['anatom_site_general_challenge'], hue=train['target'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train['diagnosis'].value_counts(normalize=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train['target'].groupby(train['diagnosis']).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.figure(figsize=(15,5))\n# sns.countplot(train['diagnosis'], hue=train['target'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" train_df = train[['sex','age_approx','anatom_site_general_challenge','diagnosis','target']]\n train_df.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ntrain_df = train_df.apply(lambda col: le.fit_transform(col.astype(str)), axis=0, result_type='expand')\ntrain_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.pairplot(train_df, hue=\"diagnosis\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(train_df.corr(),annot=True,linewidths=0.2) \nfig=plt.gcf()\nfig.set_size_inches(20,12)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_training_curves(training, validation, title, subplot):\n      if subplot%10==1: # set up the subplots on the first call\n        plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n        plt.tight_layout()\n        ax = plt.subplot(subplot)\n        ax.set_facecolor('#F8F8F8')\n        ax.plot(training)\n        ax.plot(validation)\n        ax.set_title('model '+ title)\n        ax.set_ylabel(title)\n        ax.set_xlabel('epoch')\n        ax.legend(['train', 'valid.'])\n\ncols, rows = 4, 3\ndef grid_display(list_of_images, no_of_columns=2, figsize=(15,15), title = False):\n    fig = plt.figure(figsize=figsize)\n    column = 0\n    z = 0\n    for i in range(len(list_of_images)):\n        column += 1\n        #  check for end of column and create a new figure\n        if column == no_of_columns+1:\n            fig = plt.figure(figsize=figsize)\n            column = 1\n        fig.add_subplot(1, no_of_columns, column)\n        if title:\n            if i >= no_of_columns:\n                plt.title(titles[z])\n                z +=1\n            else:\n                plt.title(titles[i])\n        plt.imshow(list_of_images[i])\n        plt.axis('off')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image_list = train[train['target'] == 0].sample(8)['image_name']\n# image_all=[]\n# for image_id in image_list:\n#     image_file = f'/kaggle/input/siim-isic-melanoma-classification/jpeg/train/'+image_id+'.jpg' \n#     img = np.array(Image.open(image_file))\n#     image_all.append(img)\n# #show_images(image_all, cols=1)\n# grid_display(image_all, 4, (15,15))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image_list = train[train['target'] == 1].sample(8)['image_name']\n# image_all=[]\n# for image_id in image_list:\n#     image_file = f'/kaggle/input/siim-isic-melanoma-classification/jpeg/train/'+image_id+'.jpg' \n#     img = np.array(Image.open(image_file))\n#     image_all.append(img)\n# grid_display(image_all, 4, (15,15))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image_list = train[train['anatom_site_general_challenge'] == 'torso'].sample(4)['image_name']\n# image_all=[]\n# for image_id in image_list:\n#     image_file = f'/kaggle/input/siim-isic-melanoma-classification/jpeg/train/'+image_id+'.jpg' \n#     img = np.array(Image.open(image_file))\n#     image_all.append(img)\n# grid_display(image_all, 4, (15,15))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image_list = train[train['anatom_site_general_challenge'] == 'lower extremity'].sample(4)['image_name']\n# image_all=[]\n# for image_id in image_list:\n#     image_file = f'/kaggle/input/siim-isic-melanoma-classification/jpeg/train/'+image_id+'.jpg' \n#     img = np.array(Image.open(image_file))\n#     image_all.append(img)\n# grid_display(image_all, 4, (15,15))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image_list = train[train['anatom_site_general_challenge'] == 'upper extremity'].sample(4)['image_name']\n# image_all=[]\n# for image_id in image_list:\n#     image_file = f'/kaggle/input/siim-isic-melanoma-classification/jpeg/train/'+image_id+'.jpg' \n#     img = np.array(Image.open(image_file))\n#     image_all.append(img)\n# grid_display(image_all, 4, (15,15))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image_list = train[train['anatom_site_general_challenge'] == 'head/neck'].sample(4)['image_name']\n# image_all=[]\n# for image_id in image_list:\n#     image_file = f'/kaggle/input/siim-isic-melanoma-classification/jpeg/train/'+image_id+'.jpg' \n#     img = np.array(Image.open(image_file))\n#     image_all.append(img)\n# grid_display(image_all, 4, (15,15))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image_list = train[train['anatom_site_general_challenge'] == 'palms/soles'].sample(4)['image_name']\n# image_all=[]\n# for image_id in image_list:\n#     image_file = f'/kaggle/input/siim-isic-melanoma-classification/jpeg/train/'+image_id+'.jpg' \n#     img = np.array(Image.open(image_file))\n#     image_all.append(img)\n# grid_display(image_all, 4, (15,15))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image_list = train[train['anatom_site_general_challenge'] == 'oral/genital'].sample(4)['image_name']\n# image_all=[]\n# for image_id in image_list:\n#     image_file = f'/kaggle/input/siim-isic-melanoma-classification/jpeg/train/'+image_id+'.jpg' \n#     img = np.array(Image.open(image_file))\n#     image_all.append(img)\n# grid_display(image_all, 4, (15,15))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image_list = train[train['diagnosis'] == 'nevus'].sample(4)['image_name']\n# image_all=[]\n# for image_id in image_list:\n#     image_file = f'/kaggle/input/siim-isic-melanoma-classification/jpeg/train/'+image_id+'.jpg' \n#     img = np.array(Image.open(image_file))\n#     image_all.append(img)\n# grid_display(image_all, 4, (15,15))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image_list = train[train['diagnosis'] == 'melanoma'].sample(4)['image_name']\n# image_all=[]\n# for image_id in image_list:\n#     image_file = f'/kaggle/input/siim-isic-melanoma-classification/jpeg/train/'+image_id+'.jpg' \n#     img = np.array(Image.open(image_file))\n#     image_all.append(img)\n# grid_display(image_all, 4, (15,15))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image_list = train[train['diagnosis'] == 'seborrheic keratosis'].sample(4)['image_name']\n# image_all=[]\n# for image_id in image_list:\n#     image_file = f'/kaggle/input/siim-isic-melanoma-classification/jpeg/train/'+image_id+'.jpg' \n#     img = np.array(Image.open(image_file))\n#     image_all.append(img)\n# grid_display(image_all, 4, (15,15))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image_list = train[train['diagnosis'] == 'lentigo NOS'].sample(4)['image_name']\n# image_all=[]\n# for image_id in image_list:\n#     image_file = f'/kaggle/input/siim-isic-melanoma-classification/jpeg/train/'+image_id+'.jpg' \n#     img = np.array(Image.open(image_file))\n#     image_all.append(img)\n# grid_display(image_all, 4, (15,15))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image_list = train[train['diagnosis'] == 'lichenoid keratosis'].sample(4)['image_name']\n# image_all=[]\n# for image_id in image_list:\n#     image_file = f'/kaggle/input/siim-isic-melanoma-classification/jpeg/train/'+image_id+'.jpg' \n#     img = np.array(Image.open(image_file))\n#     image_all.append(img)\n# grid_display(image_all, 4, (15,15))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image_list = train[train['diagnosis'] == 'solar lentigo'].sample(4)['image_name']\n# image_all=[]\n# for image_id in image_list:\n#     image_file = f'/kaggle/input/siim-isic-melanoma-classification/jpeg/train/'+image_id+'.jpg' \n#     img = np.array(Image.open(image_file))\n#     image_all.append(img)\n# grid_display(image_all, 4, (15,15))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image_list = train[train['diagnosis'] == 'atypical melanocytic proliferation'].sample(1)['image_name']\n# image_all=[]\n# for image_id in image_list:\n#     image_file = f'/kaggle/input/siim-isic-melanoma-classification/jpeg/train/'+image_id+'.jpg' \n#     img = np.array(Image.open(image_file))\n#     image_all.append(img)\n# grid_display(image_all, 4, (15,15))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image_list = train[train['diagnosis'] == 'cafe-au-lait macule'].sample(1)['image_name']\n# image_all=[]\n# for image_id in image_list:\n#     image_file = f'/kaggle/input/siim-isic-melanoma-classification/jpeg/train/'+image_id+'.jpg' \n#     img = np.array(Image.open(image_file))\n#     image_all.append(img)\n# grid_display(image_all, 4, (15,15))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image_list = train[train['diagnosis'] == 'unknown'].sample()['image_name']\n# image_all=[]\n# for image_id in image_list:\n#     image_file = f'/kaggle/input/siim-isic-melanoma-classification/jpeg/train/'+image_id+'.jpg' \n#     img = np.array(Image.open(image_file))\n#     image_all.append(img)\n# grid_display(image_all, 4, (15,15))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# arr = [15.0,20.0,25.0,30.0,35.0,40.0,45.0,50.0,55.0,60.0,65.0,70.0,75.0,80.0,85.0,90.0]\n# image_all=[]\n# titles = ['At Age 15.0','At Age 20.0','At Age 25.0','At Age 30.0','At Age 35.0','At Age 40.0'\n#           ,'At Age 45.0','At Age 50.0','At Age 55.0','At Age 60.0','At Age 65.0','At Age 70.0'\n#           ,'At Age 75.0','At Age 80.0','At Age 85.0','At Age 90.0']\n# for i in arr:\n#     image_list = train[train['age_approx'] == i].sample()['image_name']\n#     for image_id in image_list:\n#         image_file = f'/kaggle/input/siim-isic-melanoma-classification/jpeg/train/'+image_id+'.jpg' \n#         img = np.array(Image.open(image_file))\n#         image_all.append(img)\n# grid_display(image_all, 4, (15,15), title = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_list = train[train['target'] == 1].sample(2)['image_name']\nimage_all=[]\ntitles = ['original', 'Reduced Noise', \"Gaussian Blur\", 'Adjusted Contrast']\nfor image_id in image_list:\n    image_file = f'/kaggle/input/siim-isic-melanoma-classification/jpeg/train/'+image_id+'.jpg'\n    img = cv2.imread(image_file,1)\n    image_all.append(img)\n    #Reducing Noise\n    result = cv2.fastNlMeansDenoisingColored(img,None,20,10,7,21)\n    image_all.append(result)\n    #Gaussian Blur\n    blur_image = cv2.GaussianBlur(img, (7,7), 0)\n    image_all.append(blur_image)\n    #Adjusted contrast\n    contrast_img = cv2.addWeighted(img, 2.5, np.zeros(img.shape, img.dtype), 0, 0)\n    image_all.append(contrast_img)\ngrid_display(image_all, 4, (15,15), title = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image_list = train[train['target'] == 1].sample(2)['image_name']\n# image_all=[]\n# titles = ['original', 'Adaptive thresholding', \"Binary thresholding\"]\n# for image_id in image_list:\n#     image_file = f'/kaggle/input/siim-isic-melanoma-classification/jpeg/train/'+image_id+'.jpg'\n#     img = cv2.imread(image_file,1)\n#     image_all.append(img)\n#     #Adaptive Thresholding..\n#     gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n#     thresh1 = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 115, 1)\n#     image_all.append(thresh1)\n#     #Binary Thresholding...\n#     hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV) \n#     res, thresh = cv2.threshold(hsv[:, :, 0], 0, 255, cv2.THRESH_BINARY_INV)\n#     image_all.append(thresh)\n# grid_display(image_all, 3, (15,15), title = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# img = cv2.imread('/kaggle/input/siim-isic-melanoma-classification/jpeg/train/ISIC_0052212.jpg', 0)\n# # global thresholding\n# ret1,th1 = cv2.threshold(img,127,255,cv2.THRESH_BINARY)\n\n# # Otsu's thresholding\n# ret2,th2 = cv2.threshold(img,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n\n# # Otsu's thresholding after Gaussian filtering\n# blur = cv2.GaussianBlur(img,(5,5),0)\n# ret3,th3 = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n\n# # plot all the images and their histograms\n# images = [img, 0, th1,\n#           img, 0, th2,\n#           blur, 0, th3]\n# titles = ['Original Noisy Image','Histogram','Global Thresholding (v=127)',\n#           'Original Noisy Image','Histogram',\"Otsu's Thresholding\",\n#           'Gaussian filtered Image','Histogram',\"Otsu's Thresholding\"]\n# plt.figure(figsize=(15,10))\n# for i in range(3):\n#     plt.subplot(3,3,i*3+1),plt.imshow(images[i*3],'gray')\n#     plt.title(titles[i*3]), plt.xticks([]), plt.yticks([])\n#     plt.subplot(3,3,i*3+2),plt.hist(images[i*3].ravel(),256)\n#     plt.title(titles[i*3+1]), plt.xticks([]), plt.yticks([])\n#     plt.subplot(3,3,i*3+3),plt.imshow(images[i*3+2],'gray')\n#     plt.title(titles[i*3+2]), plt.xticks([]), plt.yticks([])\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Togliamo il pello\ndef hair_remove(image):\n    # convert image to grayScale\n    grayScale = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    \n    # kernel for morphologyEx\n    kernel = cv2.getStructuringElement(1,(17,17))\n    \n    # apply MORPH_BLACKHAT to grayScale image\n    blackhat = cv2.morphologyEx(grayScale, cv2.MORPH_BLACKHAT, kernel)\n    \n    # apply thresholding to blackhat\n    _,threshold = cv2.threshold(blackhat,10,255,cv2.THRESH_BINARY)\n    \n    # inpaint with original image and threshold image\n    final_image = cv2.inpaint(image,threshold,1,cv2.INPAINT_TELEA)\n    \n    return final_image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Funzione per la gestione di scala di grigi\n#l = len(hair_images[:8])\n#fig = plot.figure(figsize=(20,30))\ndef scalaGrigia(image_resize):\n    # Convert the original image to grayscale\n    #rgbScale = (cv2.cvtColor(image_resize, cv2.COLOR_BGR2RGB))\n    \n    grayScale = cv2.cvtColor(image_resize, cv2.COLOR_RGB2GRAY)\n    \n    # Kernel for the morphological filtering\n    kernel = cv2.getStructuringElement(1,(17,17))\n    \n    # Perform the blackHat filtering on the grayscale image to find the hair countours\n    blackhat = cv2.morphologyEx(grayScale, cv2.MORPH_BLACKHAT, kernel)\n    \n    # intensify the hair countours in preparation for the inpainting \n    ret,threshold = cv2.threshold(blackhat,10,255,cv2.THRESH_BINARY)\n    \n    # inpaint the original image depending on the mask\n    final_image = cv2.inpaint(image_resize,threshold,1,cv2.INPAINT_TELEA)\n    return final_image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Detect hardware\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\nexcept ValueError:\n    tpu = None\n#If TPU not found try with GPUs\ngpus = tf.config.experimental.list_logical_devices(\"GPU\")\n    \n# Select appropriate distribution strategy for hardware\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print('Running on TPU ', tpu.master())  \nelif len(gpus) > 0:\n    strategy = tf.distribute.MirroredStrategy(gpus) # this works for 1 to multiple GPUs\n    print('Running on ', len(gpus), ' GPU(s) ')\nelse:\n    strategy = tf.distribute.get_strategy()\n    print('Running on CPU')\n\n# How many accelerators do we have ?\nprint(\"Number of accelerators: \", strategy.num_replicas_in_sync)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GCS_DS_PATH = KaggleDatasets().get_gcs_path('siim-isic-melanoma-classification')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Intialize the Value...\nTRAINING_FILENAMES = tf.io.gfile.glob(GCS_DS_PATH + '/tfrecords/train*')\nTEST_FILENAMES = tf.io.gfile.glob(GCS_DS_PATH + '/tfrecords/test*')\nBATCH_SIZE = 10 * strategy.num_replicas_in_sync\nIMAGE_SIZE = [1024, 1024]\nAUTO = tf.data.experimental.AUTOTUNE\nimSize = 1024\nEPOCHS = 10\n\nVALIDATION_SPLIT = 0.18\nsplit = int(len(TRAINING_FILENAMES) * VALIDATION_SPLIT)\ntraining_filenames = TRAINING_FILENAMES[split:]\nvalidation_filenames = TRAINING_FILENAMES[:split]\nprint(\"Pattern matches {} data files. Splitting dataset into {} training files and {} validation files\"\n      .format(len(TRAINING_FILENAMES), len(training_filenames), len(validation_filenames)))\nTRAINING_FILENAMES = training_filenames","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reading Tensorflow Record...\ndef read_labeled_tfrecord(example):\n    features = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"target\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n    }\n    example = tf.io.parse_single_example(example, features)\n    image = tf.image.decode_jpeg(example['image'], channels=3)\n    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n    image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size needed for TPU\n    image = tf.image.resize(image, [imSize,imSize])\n    label = tf.cast(example['target'], tf.int32)\n    return image, label \ndef read_unlabeled_tfrecord(example):\n    u_features = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"image_name\": tf.io.FixedLenFeature([], tf.string),  # shape [] means single element\n    }\n    example = tf.io.parse_single_example(example, u_features)\n    image = tf.image.decode_jpeg(example['image'], channels=3)\n    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n    image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size needed for TPU\n    image = tf.image.resize(image, [imSize,imSize])\n    idnum = example['image_name']\n    return image, idnum # returns a dataset of image(s)\n\ndef load_dataset(filenames, labeled=True, ordered=False):\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls=AUTO)\n    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n    return dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_augment(image, label):\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    #image = tf.image.random_brightness(x, 0.2)\n    #image = cutmix(image, label)\n    return image, label  \n\ndef transform(image, inv_mat, image_shape):\n    h, w, c = image_shape\n    cx, cy = w//2, h//2\n    new_xs = tf.repeat( tf.range(-cx, cx, 1), h)\n    new_ys = tf.tile( tf.range(-cy, cy, 1), [w])\n    new_zs = tf.ones([h*w], dtype=tf.int32)\n    old_coords = tf.matmul(inv_mat, tf.cast(tf.stack([new_xs, new_ys, new_zs]), tf.float32))\n    old_coords_x, old_coords_y = tf.round(old_coords[0, :] + w//2), tf.round(old_coords[1, :] + h//2)\n    clip_mask_x = tf.logical_or(old_coords_x<0, old_coords_x>w-1)\n    clip_mask_y = tf.logical_or(old_coords_y<0, old_coords_y>h-1)\n    clip_mask = tf.logical_or(clip_mask_x, clip_mask_y)\n    old_coords_x = tf.boolean_mask(old_coords_x, tf.logical_not(clip_mask))\n    old_coords_y = tf.boolean_mask(old_coords_y, tf.logical_not(clip_mask))\n    new_coords_x = tf.boolean_mask(new_xs+cx, tf.logical_not(clip_mask))\n    new_coords_y = tf.boolean_mask(new_ys+cy, tf.logical_not(clip_mask))\n    old_coords = tf.cast(tf.stack([old_coords_y, old_coords_x]), tf.int32)\n    new_coords = tf.cast(tf.stack([new_coords_y, new_coords_x]), tf.int64)\n    rotated_image_values = tf.gather_nd(image, tf.transpose(old_coords))\n    rotated_image_channel = list()\n    for i in range(c):\n        vals = rotated_image_values[:,i]\n        sparse_channel = tf.SparseTensor(tf.transpose(new_coords), vals, [h, w])\n        rotated_image_channel.append(tf.sparse.to_dense(sparse_channel, default_value=0, validate_indices=False))\n    return tf.transpose(tf.stack(rotated_image_channel), [1,2,0])\n\ndef random_rotate(image, angle, image_shape):\n    def get_rotation_mat_inv(angle):\n        # transform to radian\n        angle = math.pi * angle / 180\n        cos_val = tf.math.cos(angle)\n        sin_val = tf.math.sin(angle)\n        one = tf.constant([1], tf.float32)\n        zero = tf.constant([0], tf.float32)\n        rot_mat_inv = tf.concat([cos_val, sin_val, zero, -sin_val, cos_val, zero, zero, zero, one], axis=0)\n        rot_mat_inv = tf.reshape(rot_mat_inv, [3,3])\n        return rot_mat_inv\n    angle = float(angle) * tf.random.normal([1],dtype='float32')\n    rot_mat_inv = get_rotation_mat_inv(angle)\n    return transform(image, rot_mat_inv, image_shape)\n\ndef get_training_dataset(dataset, do_aug=True):\n    dataset = load_dataset(TRAINING_FILENAMES, labeled=True)\n    dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n    dataset = dataset.repeat() # the training dataset must repeat for several epochs\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef get_validation_dataset(filenames, train=False):\n    dataset = load_dataset(filenames, labeled=True)\n    dataset = dataset.cache() # This dataset fits in RAM\n    if train:\n    # Best practices for Keras:\n    # Training dataset: repeat then batch\n    # Evaluation dataset: do not repeat\n        dataset = dataset.repeat()\n        dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n        dataset = dataset.shuffle(2000)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n    \ndef get_test_dataset(dataset, ordered=False):\n    dataset = load_dataset(dataset, labeled=False, ordered=ordered)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef count_data_items(filenames):\n    # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\nvalidation_dataset = get_validation_dataset(validation_filenames, train=False)\ntraining_dataset = get_training_dataset(TRAINING_FILENAMES)\ntest_dataset = get_test_dataset(TEST_FILENAMES)\nNUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\nNUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)\nNUM_VALID_IMAGES = count_data_items(validation_filenames)\nvalidation_steps = NUM_VALID_IMAGES // BATCH_SIZE\nSTEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\nprint('Dataset: {} training images, {} unlabeled test images, {} validition images'\n      .format(NUM_TRAINING_IMAGES, NUM_TEST_IMAGES, NUM_VALID_IMAGES))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data dump\nprint(\"Training data shapes:\")\nfor image, label in training_dataset.take(3):\n    print(image.numpy().shape, label.numpy().shape)\nprint(\"Training data label examples:\", label.numpy())\n\nprint(\"Validation data shapes:\")\nfor image, label in validation_dataset.take(3):\n    print(image.numpy().shape, label.numpy().shape)\nprint(\"Validation data label examples:\", label.numpy())\n\nprint(\"Test data shapes:\")\nfor image, idnum in test_dataset.take(3):\n    print(image.numpy().shape, idnum.numpy().shape)\nprint(\"Test data IDs:\", idnum.numpy().astype('U'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#     #with strategy.scope():\n#         #model = tf.keras.Sequential([\n#          #dense.DenseNet121(\n#             input_shape=(imSize, imSize, 3),\n#              weights='imagenet',\n#              include_top=False\n#          ),\n#          layers.GlobalAveragePooling2D(),\n#          layers.Dense(1, activation='sigmoid')\n#      ])\n        \n#         model.compile(\n#          optimizer='adam',\n#          loss = 'binary_crossentropy',\n#          metrics=['accuracy']\n#      )\n#     model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow.keras.applications.xception as xcep\nwith strategy.scope():\n    model = tf.keras.Sequential([\n        xcep.Xception(\n            input_shape=(imSize, imSize, 3),\n            weights='imagenet',\n            include_top=False\n        ),\n        layers.GlobalAveragePooling2D(),\n        layers.Dense(1024, activation= 'relu'), \n        layers.Dropout(0.2),\n        layers.Dense(512, activation= 'relu'), \n        layers.Dropout(0.2), \n        layers.Dense(256, activation='relu'), \n        layers.Dropout(0.2), \n        layers.Dense(128, activation='relu'), \n        layers.Dropout(0.1),\n        layers.Dense(64, activation='relu'), \n        layers.Dropout(0.1),\n        layers.Dense(1, activation='sigmoid')\n    ])\n    model.compile(\n        optimizer='adam',\n        loss = 'binary_crossentropy',\n        metrics=['accuracy']\n    )\n    model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lrfn(epoch):\n    LR_START = 0.00001\n    LR_MAX = 0.00005 * strategy.num_replicas_in_sync\n    LR_MIN = 0.00001\n    LR_RAMPUP_EPOCHS = 5\n    LR_SUSTAIN_EPOCHS = 0\n    LR_EXP_DECAY = .8\n    \n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n    return lr\n\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 10\nhistory = model.fit(training_dataset, steps_per_epoch=STEPS_PER_EPOCH, epochs=EPOCHS,validation_data=validation_dataset,callbacks=[lr_schedule])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q efficientnet","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import efficientnet.tfkeras as efn\nwith strategy.scope():\n    model = tf.keras.Sequential([\n        efn.EfficientNetB5(\n            input_shape=(imSize, imSize, 3),\n            weights='imagenet',\n            include_top=False\n        ),\n        layers.GlobalAveragePooling2D(),\n        layers.Dense(512, activation= 'relu'), \n        layers.Dropout(0.2), \n        layers.Dense(256, activation='relu'), \n        layers.Dropout(0.2), \n        layers.Dense(128, activation='relu'), \n        layers.Dropout(0.1),\n        layers.Dense(64, activation='relu'), \n        layers.Dropout(0.1),\n        layers.Dense(1, activation='sigmoid')\n    ])\n        \n    model.compile(\n        optimizer='adam',\n        loss = 'binary_crossentropy',\n        metrics=['accuracy']\n    )\n    model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_training_curves(history.history['accuracy'], history.history['val_accuracy'], 'accuracy', 211)\ndisplay_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 212)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds = test_dataset#get_test_dataset(ordered=True)\n\nprint('Computing predictions...')\ntest_images_ds = test_ds.map(lambda image, idnum: image)\nprobabilities = model.predict(test_images_ds).flatten()\nprint(probabilities)\n\nprint('Generating submission.csv file...')\ntest_ids_ds = test_ds.map(lambda image, idnum: idnum).unbatch()\ntest_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U') # all in one batch\nnp.savetxt('submission.csv', np.rec.fromarrays([test_ids, probabilities]), fmt=['%s', '%f'],\n           delimiter=',', header='image_name,target', comments='')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv(\"submission.csv\")\nsub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub['target'].hist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}